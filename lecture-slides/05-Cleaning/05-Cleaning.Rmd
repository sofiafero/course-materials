---
title: "Topic 5: Data Cleaning"
author: "Nick Hagerty <br> ECNS 460/560 <br> Montana State University"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts] 
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      fig_caption: true
---
name: toc

```{css, echo=FALSE}
# CSS for including pauses in printed PDF output (see bottom of lecture)
@media print {
  .has-continuation {
    display: block !important;
  }
}
.small {
  font-size: 90%;
}
.smaller {
  font-size: 80%;
}
.xsmall {
  font-size: 50%
}

```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(stringr.html = TRUE)
library(knitr)
knitr::opts_chunk$set(
	fig.align = "center",
	cache = TRUE,
	dpi = 300
)
```

# Table of contents

1. [Another join example](#join)

1. [Keys and relational data](#keys)

1. [Number storage](#numbers)

1. [String cleaning](#strings)

1. [Data Cleaning Checklist](#checklist)



---
class: inverse, middle
name: join

# Another join example

---

# Join example

Let's try something. Load the **nycflights13** package again.

```{r include=FALSE}
library(tidyverse)
library(nycflights13)
```

I want to calculate the average air time, distance, and number of seats for flights in this dataset. Number of seats is in the `planes` dataframe, so I have to join `planes` to `flights`.

Here is my code and output:

```{r}
flights_planes = inner_join(
  flights,
  planes |> rename(year_built = year),
  by = "tailnum"
  )
flights_planes |> 
  summarize(across(c(air_time, distance, seats), \(x) mean(x, na.rm=T)))

```

Can you find the problem here? Try to figure out how it arises and why.

































































---

# Correcting the problem

```{r}
inner_flights_planes = flights |>
  inner_join(planes |> rename(year_built = year), by="tailnum")
inner_flights_planes |> 
  summarize(across(c(air_time, distance, seats), \(x) mean(x, na.rm=T)))
```
```{r}
left_flights_planes = flights |>
  left_join(planes |> rename(year_built = year), by="tailnum")
left_flights_planes |> 
  summarize(across(c(air_time, distance, seats), \(x) mean(x, na.rm=T)))

```

Fixing it is the easy part. Why do the joins produce different answers?

---

# Diagnosing the problem

How many observations do we have in each dataset?
```{r}
nrow(flights)
nrow(inner_flights_planes)
nrow(left_flights_planes)
```
* Not all rows of `flights` matched rows in `planes`.
* Inner join completely dropped these rows, even though they still had useful data for `air_time` and `distance`.
* So the mean was taken over a different set of observations after the inner join than after the left join.

---

# Diagnosing the problem

Why exactly did these rows not match?

Let's take a closer look at the variable we joined by, `tailnum`, in the original `flights` dataset:

```{r}
summary(flights$tailnum)
```
That doesn't tell us much, since `tailnum` is a character variable.

---

# Diagnosing the problem

Why exactly did these rows not match?

Let's check more explicitly for missing values:

```{r}
flights |> 
  mutate(na_tailnum = is.na(tailnum)) |> 
  count(na_tailnum)
```

Hmm. Some rows have missing values, but 334k are non-missing, which is more than the 284k rows remaining in the inner-joined dataframe.
* So about 50,000 of these flights must have values of `tailnum` that just aren't found in `planes`.

---

# Diagnosing the problem

To see what happened even more carefully, we can add a `keep=T` option when we join.
```{r}
flights_planes = flights |>
  left_join(planes |> rename(year_built = year),
            by="tailnum",
            keep=TRUE
  )

# How many rows of flights matched a row in planes?
flights_planes |> filter(tailnum.x == tailnum.y) |> nrow()

# How many rows failed to match because tailnum was missing in flights?
flights_planes |> filter(is.na(tailnum.x)) |> nrow()

# How many rows failed to match because tailnum was not found in planes?
flights_planes |> filter(!is.na(tailnum.x) & is.na(tailnum.y)) |> nrow()

```

---

# Diagnosing the problem

Why is the mean of `seats` not affected?

--
.pull-left[
```{r}
inner_flights_planes |> 
  mutate(na_seats = is.na(seats)) |> 
  count(na_seats)
```
]
.pull-right[
```{r}
left_flights_planes |> 
  mutate(na_seats = is.na(seats)) |> 
  count(na_seats)
```
]

* `seats` does have missing values in the left-joined dataset, from rows of `flights` that did not match `planes`.

* But the non-missing values are the same ones, and when we took means, we specifically removed the missing values using `na.rm=T`.


---

# A note on Stata

(If you don't use Stata and have no plans to, you can ignore this slide!)

Already we can see some of the biggest advantages of R over Stata:

* Holds multiple dataframes in memory at the same time -- allowing us to easily examine, compare, and switch between them.
  * In Stata, you have to constantly use tempfiles along with `save` and `use`
  * Or sometimes you can get by with `preserve`/`restore`

* Holds multiple versions/stages of your data in memory at the same time.
  * In Stata, you cannot go back and look at an intermediate stage of your data without re-running everything.

* You can easily see all of your objects in the Environment tab.
  * Stata can hold non-dataframe variables (locals and globals, collectively "macros")
  * But it does not make them easy to find

---

# A note on Stata

(If you don't use Stata and have no plans to, you can ignore this slide!)

Already we can see some of the biggest advantages of R over Stata:

* Missing values are treated in a safe and sensible way.
  * NA's propagate through calculations unless you specifically choose to set `na.rm=TRUE`.
  * In Stata, missing values equal $+\infty$ (with `. < .a < .b < .c < ...`).
  * In Stata, `drop if x > 1000` will **drop all observations with missing values of x!**
  * (To fix this, ALWAYS add `drop if x>1000 & !missing(x)` to all your `if` conditions.)


---

class: inverse, middle
name: keys

# Keys & Relational Data

Images in this section are from ["R for Data Science"](https://r4ds.had.co.nz/relational-data.html) by Wickham & Grolemund, used under [CC BY-NC-ND 3.0](https://creativecommons.org/licenses/by-nc-nd/3.0/us/) and not included under this resource's overall CC license.

---

# Relational data

**Relational data**: multiple tables of data that have relationships to each other (i.e., tables that you would ever consider joining).

* Relational data is most often discussed in the context of *databases* like SQL.

* But we also use it when doing any form of data wrangling that involves joins, so it's helpful to think more formally about it.

---

# Relationships in nycflights

```{r, out.width="60%", echo=F}
include_graphics("img/relational-nycflights.png")
```

-   `flights` connects to `planes` via a single variable, `tailnum`.
-   `flights` connects to `airlines` through the `carrier` variable.
-   `flights` connects to `airports` in two ways: via the `origin` and `dest` variables.
-   `flights` connects to `weather` via `origin` (the location), and `year`, `month`, `day` and `hour` (the time).

What is the relationship between `weather` and `airports`?

---

# Keys

A **key** is a variable (or set of variables) that uniquely identifies an observation.

* In `planes`, the key is `tailnum`.
* In `weather`, the key consists of 5 variables: (`year`, `month`, `day`, `hour`, `origin`).

There are two types of keys:

* A **primary key** uniquely identifies an observation in its own data frame.
  * `planes$tailnum` is a primary key because it uniquely identifies each plane in the `planes` data frame.

* A **foreign key** uniquely identifies an observation in another data frame.
  * `flights$tailnum` is a foreign key because it appears in the `flights` data frame where it matches each flight to a unique plane.

A variable can be both a primary key *and* a foreign key.
For example, `origin` is part of the `weather` primary key, and is also a foreign key for the `airports` data frame.

---

# Keys

The primary key is the **first thing** you need to know about a new data frame.

Once you think you know the primary key, **verify it.** Here's one way to do that:
```{r}
planes |> 
  count(tailnum) |> 
  filter(n > 1)
```

--

Here's another:
```{r}
nrow(planes)
nrow(planes |> distinct(tailnum))
```


---

# Keys

You can write a **unit test** into your code to make sure this is true before proceeding:
```{r, error=TRUE}
dups_planes = planes |> 
  count(tailnum) |> 
  filter(n > 1)
stopifnot(nrow(dups_planes) == 0)

dups_weather = weather |> 
  count(year, month, day, hour, origin) |> 
  filter(n > 1)
stopifnot(nrow(dups_weather) == 0)
```

--

Alternatively:
```{r, error=TRUE}
stopifnot(nrow(planes) == nrow(planes |> distinct(tailnum)))
stopifnot(nrow(weather) == nrow(weather |> distinct(year, month, day, hour, origin)))
```


---

# Surrogate keys

What's the primary key in the `flights` data frame? Take a minute to investigate/verify.

--

You might think it would be the date + the carrier + the flight or tail number, but neither of those are unique:

```{r}
flights |> 
  count(year, month, day, carrier, flight) |> 
  filter(n > 1)
```


---

# Surrogate keys

If a data frame lacks a primary key, but it is tidy (each row is an observation), it's often useful to add in **surrogate key**:
```{r}
flights2 = flights |>
  arrange(year, month, day, carrier, flight, sched_dep_time) |>
  mutate(id = row_number()) |>
  relocate(id)
flights2
  
```

---

# Relations

A primary key and the corresponding foreign key in another data frame form a **relation**.

In general, relations are **1-to-many**: Each flight has one plane, but each plane has many flights.

* Sometimes you'll see a **1-to-1** relation, but you can think of this as a special case of 1-to-many.

* You can also find **many-to-many** relations, but you can think of these as two 1-to-many relations going in each direction.
  * There's a many-to-many relationship between airlines and airports: each airline flies to many airports; each airport hosts many airlines.

</br>

**Note on Stata: NEVER USE `merge m:m`. JUST DON'T DO IT.** There is no scenario in which it will give you what you want. This syntax should not exist. If you are tempted, you are probably either confused or looking for `joinby`.

---

# Relations

`join` does **not** think about whether your key is unique, or what type of relation you have. Instead, it simply returns all possible combinations of observations in your two dataframes:


```{r, out.width="50%", echo=F}
include_graphics("img/join-one-to-many.png")
```
```{r, out.width="60%", echo=F}
include_graphics("img/join-many-to-many.png")
```

---

# Duplicate keys

What if you join by a key that is not actually unique, when you think it is?

You'll get extra rows:

```{r}
flights_weather = flights |> 
  left_join(weather, by = c("year", "month", "day", "origin"))
nrow(flights_weather)
```

Now you no longer have a dataframe of flights.

```{r}
nrow(flights)
```


---

# A better way to join

Here's an example of a good (safe) way to join `flights` and `planes`:

```{r eval=FALSE}

# Confirm that tailnum is the primary key (unique ID) of planes
stopifnot(nrow(planes) == nrow(planes |> distinct(tailnum)))

# Join, keeping the join keys from both datasets
flights_planes = flights |>
  left_join(planes |> rename(year_built = year), by = "tailnum", keep = TRUE) |>
  rename(tailnum = tailnum.x, tailnum_planes = tailnum.y)

# Confirm the join was 1:many
stopifnot(nrow(flights) == nrow(flights_planes))

```


---
class: inverse, middle
name: numbers

# Number storage

---

# Variable types

What types of variables might you encounter in a raw dataset?

* Quantitative variables (`numeric` or `integer`).

* Dates and times (a special, reformatted type of integer).

* Categorical variables (`factor`).
  - Values are discrete and have a specific interpretation.

* Binary variables (`logical` or `integer`).
  - TRUE/FALSE or 0/1.
  - Can be thought of as a special type of categorical variable.

* Strings (`character`)
  - A sequence of letters, numbers, symbols, or other characters.

---

# Floating point problems

Simplify this expression: $1-\frac{1}{49}*49$

It's obviously 0. Now ask R:

```{r, eval=F}
1 - (1/49)*49
```
--

```{r echo=FALSE}
1 - (1/49)*49
```

This is called a **floating point** problem. It arises from the way computers store numbers.

R doesn't notice that $49/49$ simplifies to 1. It just follows the order of operations. So the first thing it does is calculate:
```{r}
(1/49)
```
Which is an irrational number. So R rounds it to 53 significant digits before multiplying by 49.

---

# Floating point problems

Most of the time, 53 digits is plenty of precision. But sometimes it creates problems.

Note: This explanation is actually too simple. The floating-point issue goes deeper than just irrational numbers. Here's another example:

```{r}
1 - 0.9 - 0.1
```

--

In 1996, a floating-point error caused a European Space Agency rocket to self-destruct 37 seconds after it was launched.

---

# Avoiding floating-point errors

Pay attention to the data type of your variables.

Avoid using logical conditions like `height == 180` for numeric variables.
* `height` may even read as `180` in the `View` window
* But under the hood, it might still be stored as `180.000000000000173...`.

What you can do instead:
* **Best option:** `dplyr::near` compares numbers with a built-in tolerance.
* Use `>` and `<` comparisons, or `between(height, 179.9, 180.1)`.
* Convert in place: `as.integer(height) == 180`
* Or with finer control: `round(height, digits=6) == 180`
* If all values are integers, store the variable as an integer in the first place.

---

# How to store a number?

**Numeric** variables are stored in scientific notation.
* Use to represent a single value, for which digits decrease in importance from left to right.
* Example: My height is `182.2469405113283` cm.

**Integer** variables lack decimal places.
* Saves memory relative to numeric variables.
* Stores values exactly, avoiding some floating-point problems.

**Character** (string) variables store the full sequence of digits literally.
* Use when digits lack quantitative information, and each digit is equally important.
* Phone numbers, credit card numbers, etc.
* No chance of the right-most digits getting lost or corrupted.

---

# More variable formats

**Dates and times** allow you to easily do math and logic on dates and times.
* See tidyverse package `lubridate`.

**Factors** allow you to store values as numbers, but *display* them as strings.
* This is useful for sorting things like month names: "Jan", "Feb", "Mar", "Apr"....
* See tidyverse packages `forcats`.

---

# Memory space

Memory space quickly becomes a problem when you work with large datasets.
* But R does a reasonably good job of handling storage efficiently.

Logical variables are smaller than integers, which are smaller than numeric.

Does it save memory to store a variable as a factor instead of a string?
* This used to be true: factor variables only store the factor labels once.
* But no longer: R uses a global string pool -- each unique string is only stored once.

`pryr::object_size()` will tell you how much memory an object takes up (accounting for shared elements within an object).


---
class: inverse, middle
name: strings

# String cleaning

Parts of this section are adapted from [“Introduction to Data Science”](http://rafalab.dfci.harvard.edu/dsbook/string-processing.html) by Rafael A. Irizarry, used under [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0).

---

# String cleaning example

Let's clean this raw data, the result of a web form in which students were asked for their height in inches:
```{r}
library(dslabs)
data(reported_heights)
str(reported_heights)
```

Unfortunately `height` is not numeric. What if we just coerce it to numeric?

---

# String cleaning example

Unfortunately `height` is not numeric. What if we just coerce it to numeric?

```{r}
heights2 = reported_heights |>
  mutate(height_num = as.numeric(height))
sum(is.na(heights2$height_num))
```

R let us do this... but we lost a lot of information.

---

# String cleaning example

Let's see some examples of non-numeric entries:
```{r}
heights_probs = filter(heights2, is.na(height_num))
View(heights_probs)
x = heights_probs$height
x
```

---

# String cleaning example

Many of these entries have valuable information, so let's try to salvage as much as we can.

**The general way to proceed is:**
1. Identify the most common patterns among the problematic entries.
2. Write an algorithm to correct these.
3. Review results to make sure your algorithm worked correctly.
4. Look at the remaining problematic entries. Tweak your algorithm or add another one.
5. Stop when all useful information is corrected (or when MB < MC).

**What are the most common patterns?**

--
* Strings of the form `x'y` or `x'y"` where `x` is feet and `y` is inches.
* Strings of the form `x ft y inches`, except that "ft" and "inches" are inconsistent.

**My approach is going to be to:**
1. Try to convert everything to the pattern `x y`.
2. `separate` the feet and inches values.
3. Calculate total inches from feet and inches.

---

# String cleaning example

Start by replacing 4 punctuation marks with spaces (note we have to **escape** the "):
```{r}
x2 = x |>
  str_replace_all("'", " ") |>
  str_replace_all(",", " ") |>
  str_replace_all("\"", " ") |>
  str_replace_all(",", " ")
x2
```

---

# String cleaning example

We can make this more concise by using **regular expressions** (more on this soon):
```{r}
x2 = x |>
  str_replace_all("'|,|\"|,", " ")
x2
```

---

# String cleaning example

Also get rid of some common words, and **trim** extra spaces:
```{r}
x2 = x |>
  str_replace_all("'|,|\"|,|ft|feet|inches|and", " ") |>
  str_trim()
x2
```

---

# String cleaning example

Also remove extra spaces **within** a string:
```{r}
x2 = x |>
  str_replace_all("'|,|\"|,|ft|feet|inches|and", " ") |>
  str_squish()
x2
```

---

# String cleaning example

A few more tweaks:
```{r}
x2 = x |>
  str_replace_all("'|,|\"|,|\\*|ft|feet|inches|and", " ") |>
  str_squish() |>
  str_replace(" \\.", " ")
x2
```

---

# String cleaning example

This looks pretty good, so let's apply this code to the original data:
```{r}
heights3 = reported_heights |>
  # Preserve original height column
  rename(height_orig = height) |>
  mutate(height_clean = height_orig) |>
  # Clean height values
  mutate(
    height_clean = str_replace_all(height_clean, "'|,|\"|,|\\*|ft|feet|inches|and", " "),
    height_clean = str_squish(height_clean),
    height_clean = str_replace_all(height_clean, " \\.", " ")
    )

# Calculate total inches
heights4 = heights3 |>
  # Separate feet and inches into 2 columns
  separate(height_clean, sep = " ", into = c("feet", "inches"), fill = "left") |>
  # Coerce them to numeric
  mutate(across(c(feet, inches), as.numeric)) |>
  # Replace NAs in feet with 0's
  mutate(feet = replace_na(feet, 0)) |>
  # Calculate new height
  mutate(height_clean = feet * 12 + inches)
```

---

# String cleaning example

Great. Now let's go back and look at the remaining non-numeric values:
```{r}
filter(heights4, is.na(height_clean))
```

Only 5 left. We could do a bit more with them, but let's set them aside for now.

Next, look at the numerical range of height values. Do they make sense?
```{r}
summary(heights4$height_clean)
```

---

# String cleaning example

If you `View` the data (I recommend sorting with `arrange(height_clean)`), you'll find:
* Many values between 5 and 7 which are clearly in **feet** instead of inches.
* Many values between 150 and 214 which are clearly in **cm** instead of inches.

```{r}
heights5 = heights4 |>
  mutate(height_clean = case_when(
    # Convert values in feet
    height_clean >= 5 & height_clean <= 7 ~ height_clean * 12,
    # Convert values in cm
    between(height_clean, 150, 214) ~ height_clean / 2.54,
    # Otherwise keep same value
    TRUE ~ height_clean
  )) |>
  arrange(height_clean)
```

---

# String cleaning example

Now, how many values are outside of plausible values?
```{r}
heights5 |>
  mutate(ok = between(height_clean, 3.5*12, 7.5*12)) |>
  count(ok)
```

1. Some of these remaining values may still contain interpretable information. **There may be more cleaning to do.**

1. Some of them may not, in which case we probably won't use them for analysis. **But don't discard them yet!** We'll come back to extreme values (aka outliers) in a couple of weeks.

1. You'll find there are also a few instances where our cleaned value appears sensible, but the original value does not. **You may need to tweak the algorithm further.**


---

# Regular expressions

Regular expressions are code to describe patterns within strings. They are not specific to R but work across basically all programming languages.

```{r, eval = F}
names = c("Python", "SPSS", "Stata", "Julia")

# Match strings that CONTAIN a lowercase "t"
str_view_all(names, "t")
```

```{r, echo = F}
names = c("Python", "SPSS", "Stata", "Julia")

# Match strings that CONTAIN a lowercase "t"
str_view_all(names, "t", html = T)
```

---

# Common regular expressions

.pull-left[
Match strings that START with a capital "S":
```{r, eval = F}
str_view_all(names, "^S")
```
```{r, echo = F}
str_view_all(names, "^S", html = T)
```
]
.pull-right[
Match strings that END with a lowercase "a":
```{r, eval = F}
str_view_all(names, "a$")
```
```{r, echo = F}
str_view_all(names, "a$", html = T)
```
]

`^` and `$` are called **anchors**.

---

# Common regular expressions

.pull-left[
Match all lowercase vowels:
```{r, eval = F}
str_view_all(names, "[aeiou]")
```
```{r, echo = F}
str_view_all(names, "[aeiou]", html = T)
```
]

.pull-right[
Match everything BUT lowercase vowels:
```{r, eval = F}
str_view_all(names, "[^aeiou]")
```
```{r, echo = F}
str_view_all(names, "[^aeiou]", html = T)
```
]

---

# Common regular expressions

.pull-left[
Use a vertical bar for "or":
```{r, eval = F}
str_view_all(names, "Stata|SPSS")
```
```{r, echo = F}
str_view_all(names, "Stata|SPSS", html = T)
```
]

.pull-right[
And parentheses to clarify:
```{r, eval = F}
str_view_all(names, "S(tata|PSS)")
```
```{r, echo = F}
str_view_all(names, "S(tata|PSS)", html = T)
```
]

---

# Last remarks on regular expressions

**All kinds of regex cheat sheets and interactive testers are available via a quick Google.**

Regexps are hard to read and troubleshoot. Try not to get too deep into them -- you can often accomplish the same goal by breaking it up into smaller chunks.

> Some people, when confronted with a problem, think "I know, I’ll use regular expressions." Now they have two problems. - Jamie Zawinski

---

# Last remarks on regular expressions

This is (the start of) a real regular expression that checks whether an email address is valid:

`(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\<(?:(?:\r\n)?[ \t])*(?:@(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\>(?:(?:\r\n)?[ \t])*)|(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*:(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\<(?:(?:\r\n)?[ \t])*(?:@(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\>(?:(?:\r\n)?[ \t])*)(?:,\s*(?:(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\<(?:(?:\r\n)?[ \t])*(?:@(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\>(?:(?:\r\n)?[ \t])*))*)?;\s*)`


---

# Useful functions for cleaning strings

`stringr` functions we've used here:
* `str_replace` and `str_replace_all`: Replace parts of strings.
* `str_trim` and `str_squish`: Remove extra spaces.
* `str_view_all`: Illustrates matches, to help develop regular expressions.

Other tidyverse functions we've used:
* `between`: Test whether values fall within a numerical range.
* `case_when`: Multiple conditional expressions.
* `replace_na`: Set missing values to a certain value.

Other useful `stringr` functions:
* `str_sub`: Subset strings by position of characters.
* `str_detect`: Test whether a string matches a pattern.

Other useful tidyverse functions:
* `na_if`: Set a certain value to missing.
* `bind_rows`: Append two datasets that have the same variable structure.



---
class: inverse, middle
name: checklist

# Data Cleaning Checklist


---

# Data Cleaning Checklist

Neither exhaustive nor necessary for all purposes, but a good starting point.

***

1. **Convert file formats** as necessary, and import your data.

1. **Structure data into tidy format** if not already.

1. **Remove irrelevant, garbage, or empty** columns and rows.

1. **Identify the primary key**, or define a surrogate key.

1. **Resolve duplicates** (remove true duplicates, or redefine the primary key).

1. **Understand the definition, origin, and units** of each variable, and document as necessary.

1. **Rename variables** as necessary, to be succinct and descriptive.

---

# Data Cleaning Checklist

Neither exhaustive nor necessary for all purposes, but a good starting point.

***

<ol start=8>

<li><b>Convert variable formats</b> as necessary:

<ul>
   <li> Numeric variables -- may be inappropriately stored as strings when there are typos.
   <li> Dates and times -- store in date or time format.
   <li> Binary variables -- code as 0/1 integers (not "Yes"/"No" or 1/2).
   <li> Factors -- use when strings take a limited set of possible values.
   <li> ID variables -- store as integers or character, not numeric.
   <li> Strings of digits -- store as character, not numeric.
</ul>
<br>

<li><b>Understand patterns of missing values.</b>

<ul>
   <li> Find out why they're missing.
   <li> Make sure they are not more widespread than you expect.
   <li> Convert other intended designations (i.e., -1 or -999) to NA.
   <li> Distinguish between missing values and true zeros.
</ul>
<br>

</ol>

---

# Data Cleaning Checklist

<ol start=10>

<li><b>Make units and scales consistent.</b> Avoid having in the same variable:

<ul>
   <li> Some values in meters and others in feet.
   <li> Some values in USD and others in GBP.
   <li> Some percentages as 40% and others as 0.4.
   <li> Some values as millions and others as billions.
</ul>
<br>

<li><b>Enforce logical conditions on quantitative variables.</b>

<ul>
   <li> Define any range restrictions each variable should satisfy, and check them.
   <li> Correct any violations that are indisputable data entry mistakes.
   <li> Create a flag variable to mark remaining violations.
</ul>
<br>
 
<li><b>Clean string variables</b> as needed. Some common operations:

<ul>
   <li> Make entirely uppercase or lowercase
   <li> Remove punctuation
   <li> Trim spaces (extra, starting, ending)
   <li> Ensure order of names is consistent
   <li> Remove uninformative words like "the" and "a"
   <li> Correct spelling inconsistencies (consider text clustering packages)
</ul>
<br>

</ol>

---

# Data Cleaning Checklist

Neither exhaustive nor necessary for all purposes, but a good starting point.

***

<ol start=13>

<li><b>Save your clean data</b> to disk before further manipulation (merging data, transforming variables, restricting the sample). Think of the whole wrangling/cleaning/analysis pipeline as 2 distinct phases:

<ul>
   <li> Taking messy data from external sources and making a nice, neat table that you are likely to use for multiple purposes in analysis.
   <li> Taking that nice, neat table and doing all kinds of new things with it.
</ul>
<br>

</ol>

---

# Best practices for data cleaning

1. **Record all steps in a script.**

--

1. **Never overwrite the original raw data file.**

--

1. **When editing values, identify observations by substantive logical conditions** rather than by observation ID or (even worse) row number. You want the changes you make to be rule-based, for 2 reasons:
   * So that they're general -- able to handle upstream changes to the data.
   * So that they're principled -- no one can accuse you of cherry-picking.

--

1. **Look at your data** every step of the way, to spot issues you haven't thought of, and to make sure you're actually doing what you think you're doing.

---

# Summary

1. [Another join example](#join)

1. [Keys and relational data](#keys)

1. [Number storage](#numbers)

1. [String cleaning](#strings)

1. [Data Cleaning Checklist](#checklist)


