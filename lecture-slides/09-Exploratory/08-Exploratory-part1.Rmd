---
title: "Topic 8: Exploratory Analysis"
subtitle: "Part 1: Understand individual variables"
author: "Nick Hagerty <br> ECNS 460/560 <br> Montana State University"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      fig_caption: true
---
name: toc

```{css, echo = FALSE}
# CSS for including pauses in printed PDF output (see bottom of lecture)
@media print {
  .has-continuation {
    display: block !important;
  }
}
.remark-code-line {
  font-size: 95%;
}
.small {
  font-size: 75%;
}
.medsmall {
  font-size: 90%;
}
.scroll-output-full {
  height: 90%;
  overflow-y: scroll;
}
.scroll-output-75 {
  height: 75%;
  overflow-y: scroll;
}
```

```{r setup, include = FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
library(tidyverse)
knitr::opts_chunk$set(
	fig.align = "center",
	cache = TRUE,
	dpi = 300,
  warning = F,
  message = F
)
```

# Table of contents

1. [Get to know your data](#first)

1. [Describe your categorical variables](#categories)

1. [Describe your continuous distributions](#distributions)

1. [Handle your extreme values](#extreme)

1. [Choose whether to transform variables](#transform)


---

# Why?

**Before you analyze your data, you need to understand your data.**
- Never rush into regressions or other fancy analysis.
- You will often get wrong or misleading results!

--

**What's the point of the steps we'll cover today?**
- Verify the data you have is the data you expected.
- Detect errors in data cleaning.
- Learn surprising new features of the context you're studying.
- Make more appropriate decisions in downstream analysis.
- Better interpret your results (what's driving them?).

--

**Do I really have to do these steps for every variable in my data?**
- Only for the ones that you want to give you the right answers.
- Especially crucial for your **outcome** and **treatment** variables.
- Still important for other (e.g., control) variables.
- Maybe less critical for *some* types of prediction (ML) techniques.


---
class: inverse, middle
name: first

# Get to know your data

---

# Setup

Load the tidyverse if necessary:
```{r eval = FALSE}
library(tidyverse)
```

Download this data on hotel listings in Vienna, Austria in November 2017:
```{r}
vienna = read_csv("https://osf.io/y6jvb/download")
```

.small[Data from [Gabors Data Analysis](https://osf.io/7epdj/) by GÃ¡bor BÃ©kÃ©s and GÃ¡bor KÃ©zdi, used under [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/).]

---

# First look

You already know these, but let's go over them:

```{r}
head(vienna)
```

---

# First look

.scroll-output-full[
```{r}
View(vienna)

summary(vienna)
```
]

---

# Better summaries with skimr

```{r, eval = F}
install.packages("skimr")
library(skimr)
skim(vienna)
```
.scroll-output-75[
  .small[
```{r echo = FALSE}
library(skimr)
skim(vienna)
```
  ]
]

---

# Better summaries with skimr

```{r, eval = F}
vienna |> 
  mutate(stars = factor(stars)) |>
  skim()
```
.scroll-output-75[
  .small[
```{r echo = FALSE}
vienna |> 
  mutate(stars = factor(stars)) |>
  skim()
```
  ]
]


---
class: inverse, middle
name: categories

# Describe your categorical variables

---

# Frequency tables

Base R:

```{r}
table(vienna$stars, useNA = "ifany")
```

</br>

Literally just the basics. Not terribly informative, well-formatted, or tidy.

---

# Frequency tables

Using the tidyverse (we've done this before):

```{r}
vienna |> count(stars)
```

</br>

Advantage: output is a tibble; can be piped elsewhere.

---

# Frequency tables

Using `summarytools::freq`:

```{r, eval = F}
install.packages("summarytools")
```
```{r}
library(summarytools)
freq(vienna$stars)
```

---

# Frequency tables

Using `summarytools::freq`:

```{r, eval = F}
install.packages("summarytools")
```
```{r}
library(summarytools)
freq(vienna$stars, order = "freq")
```

---

# Crosstabs (two-way frequency tables)

Using `summarytools::ctable`:

```{r}
ctable(vienna$city_actual, as_factor(vienna$scarce_room))
```

Default percentages are out of each row. Options to make them by column or table.

--

Could we make a similar table with `dplyr` and `tidyr`?

---

# Crosstabs (two-way frequency tables)

Yes, but it's fairly complicated and not nicely formatted...

```{r message = FALSE}
vienna |>
  group_by(city_actual, scarce_room) |>
  summarize(n = n()) |>
  mutate(percent = n/sum(n)) |>
  pivot_wider(names_from = scarce_room, values_from = n:percent) |>
  mutate(across(n_0:percent_1, replace_na, 0)) |>
  rowwise() |>
  mutate(row_sum = sum(c_across(contains("n"))))

```

---

# Bar plots

We're going to use `ggplot2` to make graphs. Don't worry too much about the syntax yet; we'll talk about it more in the unit on visualization (coming soon!).

```{r, out.width = "80%", fig.height = 4}
ggplot(vienna, aes(y = neighbourhood)) + 
  geom_bar()
```


---
class: inverse, middle
name: distributions

# Describe your distributions

---

# Histograms

With default settings:

```{r message = FALSE, warning = FALSE, out.width = "80%", fig.height = 4}
ggplot(vienna, aes(price)) + 
  geom_histogram()
```

---

# Histograms

Make bins line up with nice round numbers:

```{r, out.width = "80%", fig.height = 4, message = FALSE, warning = FALSE}
ggplot(vienna, aes(price)) + 
  geom_histogram(boundary = 0, binwidth = 25)
```

---

# Histograms

Too much detail? Use a larger bin width:

```{r, out.width = "80%", fig.height = 4, message = FALSE, warning = FALSE}
ggplot(vienna, aes(price)) + 
  geom_histogram(boundary = 0, binwidth = 100)
```

---

# Histograms

Want more detail? Use a smaller bin width:

```{r, out.width = "80%", fig.height = 4, message = FALSE, warning = FALSE}
ggplot(vienna, aes(price)) + 
  geom_histogram(boundary = 0, binwidth = 1)
```

---

# Kernel density plots

A smoothed version of a histogram.

```{r, out.width = "80%", fig.height = 4, message = FALSE, warning = FALSE}
ggplot(vienna, aes(rating)) + 
  geom_density()
```

---

# Kernel density plots

The **bandwidth** controls the degree of smoothing. Smaller:

```{r, out.width = "80%", fig.height = 4, message = FALSE, warning = FALSE}
ggplot(vienna, aes(rating)) + 
  geom_density(adjust = .25)
```

---

# Kernel density plots

The **bandwidth** controls the degree of smoothing. Larger:

```{r, out.width = "80%", fig.height = 4, message = FALSE, warning = FALSE}
ggplot(vienna, aes(rating)) + 
  geom_density(adjust = 2)
```

---

# Kernel density plots

Show multiple groups:

```{r, out.width = "80%", fig.height = 4, message = FALSE, warning = FALSE}
vienna |>
  mutate(stars_rounded = factor(round(stars))) |>
  ggplot(aes(rating, color = stars_rounded)) + 
    geom_density(adjust = 2)
```

---

# Kernel density plots

Show multiple groups:

```{r, out.width = "80%", fig.height = 4, message = FALSE, warning = FALSE}
vienna |>
  mutate(stars_rounded = factor(round(stars))) |>
  ggplot(aes(rating, fill = stars_rounded)) + 
    geom_density(adjust = 2, alpha = 0.4)
```

---

# Kernel density plots

When might you prefer a density plot vs. a histogram?

--

Histogram:
- Want to see your raw data as literally as possible
- Want to count extreme values
- Care about thresholds

Density plot:
- Want a more general idea of the distribution
- Want to compare tendencies of distributions

---

# Kernel density plots

How does the smoothing work?

```{r, out.width = "60%", echo = F}
include_graphics("img/kernel-density-animation.gif")
```
.small[Image by [David Robinson](http://varianceexplained.org/files/bandwidth.html) not included under the CC license.]

---

# Bias-variance tradeoff

Bandwidth choice illustrates a **bias-variance tradeoff**.

Smaller bandwidth:
- Less bias (more literal representation of your raw data).
- But higher variance (how meaningful are those wiggles?).

```{r echo = FALSE, out.width = "80%", fig.height = 4, message = FALSE, warning = FALSE}
ggplot(vienna, aes(rating)) + 
  geom_density(adjust = .25)
```

---

# Bias-variance tradeoff

Bandwidth choice illustrates a **bias-variance tradeoff**.

Larger bandwidth:
- Lower variance (smoother lines).
- But more bias (less directly showing your raw data).

```{r echo = FALSE, out.width = "80%", fig.height = 4, message = FALSE, warning = FALSE}
ggplot(vienna, aes(rating)) + 
  geom_density(adjust = 2)
```

---

# Bias-variance tradeoff

Bias is not always bad! Often we want to accept some bias (inaccuracy) in exchange for less variance (more precision).

```{r, out.width = "60%", echo = F}
include_graphics("img/bias-variance.png")
```
.small[Image by [Scott Fortmann-Roe](http://scott.fortmann-roe.com/docs/BiasVariance.html) not included under the CC license.]

---
class: inverse, middle
name: extreme

# Handle your extreme values

---

# Extreme values

There are a couple of really high prices in the `price` variable.

Our histogram would look a lot nicer if we could get rid of them...

```{r, out.width = "80%", fig.height = 4, message = FALSE, warning = FALSE}
ggplot(vienna, aes(price)) + 
  geom_histogram(boundary = 0, binwidth = 25)
ggplot(vienna, aes(distance)) + 
  geom_histogram()
```

---

# Extreme values

**Should we get rid of them? How do we decide?**

--

> ðŸ—£I donâ€™t know who needs to hear this but we âœ¨donâ€™tâœ¨ get rid of outliers *because* theyâ€™re extreme...<br><br>we get rid of them when their extreme-ness indicates theyâ€™re not a part of the data generating process we want to study (like a typo that says your newborn is 1000 lbs) </p>&mdash; Chelsea Parlett-Pelleriti (@ChelseaParlett) <a href = "https://twitter.com/ChelseaParlett/status/1356285012375556109?ref_src = twsrc%5Etfw">February 1, 2021</a>

---

# Extreme values

Values that are much larger or smaller than the rest of your distribution, or that fail logical checks, should be investigated until you are able to classify them into one of these categories:
1. They are **erroneous** (and can be corrected, or else excluded).
2. They are part of a **different** data generating process (and should be excluded).
3. They are **correct** and produced by the same process as less extreme values (and should be retained).

Making sound judgments about extreme values requires **domain knowledge**.
- If you don't know yourself, it's time to ask someone else! Go back to the documentation of your raw data, or contact the person who collected or gave you the data.
- This can be one of the most labor-intensive aspects of data analysis, but it is critical to ensure good data quality and accurate conclusions.

---

# Extreme values

Even when extreme values are correct and truly belong to your distribution, they can exert inordinate influence on your analysis. Your results may reflect the extreme values more than the **central tendency** of your data.
- But this alone is ***not a reason*** to remove extreme values.
- Instead, it's a sign that you should consider applying a **transformation** to your variable.


---
class: inverse, middle
name: transform

# Choose whether to transform variables

---

# Transformations

Take a look at the `rating_count` variable.
```{r, out.width = "80%", fig.height = 4, message = FALSE, warning = FALSE}
ggplot(vienna, aes(rating_count)) +
  geom_histogram()
```

---

# Transformations

We can get a different view of this distribution by applying a (natural) logarithmic transformation:

```{r, out.width = "80%", fig.height = 4, message = FALSE, warning = FALSE}
vienna = mutate(vienna, ln_rating_count = log(rating_count))
ggplot(vienna, aes(ln_rating_count)) + 
  geom_histogram()
```

---

# Transformations

Now we can see finer differences among values in the left (bottom) of the distribution... at the expense of compressing values in the right (top) of the distribution. 

Many common and important variables tend to follow approximately lognormal distributions (e.g., income, landholdings, trade quantities).
- I.e., they are right-skewed before taking the log, but normally distributed afterward.

**Which should we prefer,** the raw variable or the log-transformed variable?

--

For right-skewed data, log-transformed variables...

1. Are **less sensitive to extremely large values.**
2. Better reflect the **central tendency** (the mean is closer to the median and mode).

--

These are nice properties, but not always the best reason to transform (or not). Two other considerations are even more fundamental:
1. What type of **variation** you care about most.
2. What type of **data generating process** you think created the variation.

---

# Transformations

**1. What type of variation do you care about more: level changes or proportional changes?**

**Levels:** Each tick on the raw histogram **increments** # of ratings by the same **number.**
- 10 $\rightarrow$ 20 matters equally as 110 $\rightarrow$ 120. 100 $\rightarrow$ 200 matters much more.

**Logs:** Each tick on the log-transformed histogram **multiplies** # of ratings by the same **factor.**
- 10 $\rightarrow$ 20 matters equally as 100 $\rightarrow$ 200. 110 $\rightarrow$ 120 matters much less.
- Why? If $\log(x_1) - \log(x_0) = c$, then $x_1 = k x_0$ (where $k \equiv e^c$).

```{r echo = FALSE, fig.height = 3, message = FALSE, warning = FALSE, out.width = "80%"}
ggplot(vienna, aes(rating_count)) +
  geom_histogram() +
  scale_x_log10()
```

---

# Transformations

**1. What type of variation do you care about more: level changes or proportional changes?**

Often, proportional changes seem more policy-relevant than level changes (everyone's income increases by 2% vs. by $1000).

We are going to use this variation to learn about how this variable relates to other variables -- so we want to match the variation we're working with to the variation we care about.

</br>

```{r echo = FALSE, fig.height = 3, message = FALSE, warning = FALSE, out.width = "80%"}
ggplot(vienna, aes(rating_count)) +
  geom_histogram() +
  scale_x_log10()
```

---

# Transformations

**2. Is the variation created by an additive or multiplicative process?**

We are going to use this **variation** to learn about how this variable relates to other variables -- so we want to match the actual data generating process as closely as possible.
- Estimates are likely to be more precise.
- Their interpretation is likely to be more meaningful.

Often, proportional effects seem more likely than level effects (a policy increases everyone's income by 2% vs. by $1000).

--

Example of an **additive** process:
$$Rating = 4 + 0.2 \cdot Clean + 0.3 \cdot ListingAccurate$$

--

Example of a **multiplicative** process:
$$CountRatings = CountStays \cdot \textrm{30% give ratings}$$
$$\log(CountRatings) = \log(CountStays) + \log(0.3)$$

---

# Transformations

**2. Is the variation created by an additive or multiplicative process?**

Income is likely a chiefly **multiplicative** process:

$$Income = I_0 + I_0 \cdot (5\%)^{Experience}$$
$$Income = I_0 \cdot (1.05)^{Experience}$$
$$Income = I_0 \cdot (1.05)^{Experience} \cdot (1.2)^{GradDegree}$$

--

Taking logs lets us represent it linearly:

$$\log(Income) = \log(I_0) + \log(1.05^{Experience}) + \log(1.2^{GradDegree})$$
$$\log(Income) = \log(I_0) + \log(1.05)\cdot Experience + \log(1.2) \cdot GradDegree$$
$$\log(Income) \approx \log(I_0) + 0.05\cdot Experience + 0.2 \cdot GradDegree$$
(using the approximation $\log(1 + x) \approx x$ for small $x$).


---

# Transformations

**Which type of logarithm should you use?**

- For visualizing data, the base-10 logarithm is easiest to interpret.

- For regression estimation, the natural log has the nice property that coefficients can be directly interpreted as approximate percentage changes.
   * Allows us to directly estimate elasticities, which we like because they're unitless and convenient in many theoretical models.

- Does not matter for your results, just their interpretation (they are equal up to a constant).

---

# Transformations

One other common transformation: **z-score normalization.**

$$ z_i = \frac{x_i-\mu}{\sigma} $$

where $\mu$ is the variable's mean and $\sigma$ is its standard deviation.

- Centers the variable at 0.
- One unit of a z-score = one standard deviation.

In causal inference, often used for variables that have no intrinsic quantitative meaning.
- E.g., student test scores in education.

In prediction, often required for inputs to many machine learning algorithms.
- Avoids floating point problems by putting all variables on similar scales.


---

# Transformations

**What about other transformations?**

Rarer in economics, because they are harder to interpret.

- Box-Cox, square root, exponential.
- Normalization on the (0, 1) interval: max/min, sigmoidal, hyperbolic tangent.

Avoid unless you have a strong reason for using a specific one.

---

# Summary of Part 1

### Understand each of your variables
* Get a initial **summary** of your dataset with `skimr::skim`.
* Explore categorical variables with **frequency tables** and **crosstabs.**
* For numerical variables, look at the **histograms** before you do anything else.
* The bandwidth of **kernel density** plots features a bias/variance tradeoff.

### Data handling decisions
* **Extreme values** call for scrutiny but should be excluded only if not created by the data generating process you want to study.
* Logarithmic **transformations** are widely useful for describing right-skewed variables.

